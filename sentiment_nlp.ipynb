{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from os import listdir\n",
    "import pickle\n",
    "import re\n",
    "import copy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class variables\n",
    "#List of words\n",
    "# neg_token = list()\n",
    "# pos_token = list()\n",
    "neg_vocab = Counter() #counter, so that frequency will be added not replaced\n",
    "pos_vocab = Counter()\n",
    "# this dict will store words without any common words.\n",
    "neg_dict = dict()\n",
    "pos_dict = dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "def load_data(filename):\n",
    "    #open text file\n",
    "    with codecs.open(filename, 'r', encoding = 'utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "def clean_data(text):\n",
    "    word_pattern = re.compile(\"^\\w+$\")\n",
    "    esw = stopwords.words('english')\n",
    "    esw.append(\"would\")\n",
    "    \n",
    "    #tokenize, stemming, converting to lower...\n",
    "    tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in esw]\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "#     print(type(tokens))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate vocabulary\n",
    "def gen_vocab(filename, vocab):\n",
    "    text = load_data(filename)\n",
    "    tokens = clean_data(text)\n",
    "    #update tokens with their frequency as a dict but doesnt replace old value like dict\n",
    "    vocab.update(tokens) #Counter-helps to add frequency for same word unlike dict-replace the old value with new\n",
    "    if vocab == neg_vocab:\n",
    "        neg_token.append(tokens)\n",
    "        \n",
    "    elif vocab == pos_vocab:\n",
    "        pos_token.append(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run all the text documents of the directory and call func to update vocabulary\n",
    "def run_all_docs(directory, vocab):\n",
    "    #run all the text file here through loop\n",
    "    for filename in listdir(directory):\n",
    "        #leave the file it's not a text file\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        #make complete path    \n",
    "        path = directory + '/' + filename\n",
    "        gen_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after making vocab with counter(freq), it will remove common word\n",
    "def remove_common_words(dict_to_clear, dict_to_refer):\n",
    "    u_word = dict()\n",
    "    for k,v in dict_to_clear.items():\n",
    "        if (k not in dict_to_refer) or ((k in dict_to_refer) and (v > dict_to_refer[k])):\n",
    "            u_word[k] = v\n",
    "    return u_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pkl(vocab):\n",
    "    if vocab == neg_vocab:\n",
    "        #save Vocab with their freuency in dataframe as pkl\n",
    "        neg_df_no_comm = pd.DataFrame.from_dict(neg_dict, orient = 'index').reset_index()\n",
    "        neg_df_no_comm.columns = ['word', 'count']\n",
    "        neg_df_no_comm['type'] = 0\n",
    "        neg_df_no_comm.to_pickle(\"C:/Users/family/Desktop/Sentiment_NLP/Dictionary/neg_df_no_comm.pkl\")\n",
    "        \n",
    "    elif vocab == pos_vocab:\n",
    "        #save Vocab with their freuency in dataframe as pkl\n",
    "        pos_df_no_comm = pd.DataFrame.from_dict(pos_dict, orient = 'index').reset_index()\n",
    "        pos_df_no_comm.columns = ['word', 'count']\n",
    "        pos_df_no_comm['type'] = 1\n",
    "        pos_df_no_comm.to_pickle(\"C:/Users/family/Desktop/Sentiment_NLP/Dictionary/pos_df_no_comm.pkl\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #To build a dataframe of vocabulary with column \"type\" where o = negative and 1 =positive\n",
    "# def build_pkl(vocab):\n",
    "    \n",
    "#     if vocab == neg_vocab:\n",
    "#         #save Vocab with their freuency in dataframe as pkl\n",
    "#         neg_df = pd.DataFrame.from_dict(neg_vocab, orient = 'index').reset_index()\n",
    "#         neg_df.columns = ['word', 'count']\n",
    "#         neg_df['type'] = 0\n",
    "#         neg_df.to_pickle(\"C:/Users/family/Desktop/Sentiment_NLP/neg_df.pkl\")\n",
    "               \n",
    "#         #pickle the list of tokens\n",
    "#         f = open(\"C:/Users/family/Desktop/Sentiment_NLP/neg_token.pkl\", 'wb')\n",
    "#         pickle.dump(neg_token, f)\n",
    "#         f.close()\n",
    "        \n",
    "#     elif vocab == pos_vocab:\n",
    "#         #save Vocab with their freuency in dataframe as pkl\n",
    "#         pos_df = pd.DataFrame.from_dict(pos_vocab, orient = 'index').reset_index()\n",
    "#         pos_df.columns = ['word', 'count']\n",
    "#         pos_df['type'] = 1\n",
    "#         pos_df.to_pickle(\"C:/Users/family/Desktop/Sentiment_NLP/pos_df.pkl\")\n",
    "                        \n",
    "#         #pickle the list of tokens\n",
    "#         f = open(\"C:/Users/family/Desktop/Sentiment_NLP/pos_token.pkl\", 'wb')\n",
    "#         pickle.dump(pos_token, f)\n",
    "#         f.close()        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_vocab(path,type):\n",
    "    if type == \"neg\":\n",
    "        vocab = neg_vocab\n",
    "        run_all_docs(path, vocab)         \n",
    "        neg_dict = (remove_common_words(neg_vocab, pos_vocab))#remove common words and make a dictionary\n",
    "        build_pkl(vocab)\n",
    "    elif type == \"pos\":\n",
    "        vocab = pos_vocab\n",
    "        run_all_docs(path, vocab)\n",
    "        pos_dict = (remove_common_words(pos_vocab, neg_vocab))#remove common words and make a dictionary \n",
    "        build_pkl(vocab)\n",
    "    else:\n",
    "        print(\"Wrong vocabulary type \\nPlease enter either neg or pos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#physical path of text file which have movie reviews \n",
    "neg_path =\"C:/Users/family/Desktop/Sentiment_NLP/Data/txt_sentoken/neg\"\n",
    "pos_path =\"C:/Users/family/Desktop/Sentiment_NLP/Data/txt_sentoken/pos\"\n",
    "\n",
    "# it's taking physical path of txt file and type of sentiment as a parameter\n",
    "sentiment_vocab(pos_path, \"pos\") \n",
    "sentiment_vocab(neg_path, \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making DataFrame which doesnt have common words in both (negative and postive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15835 20246\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_dict), len(pos_dict)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(neg_df.shape)\n",
    "# print(pos_df.shape)\n",
    "# print(neg_df_no_comm.shape)\n",
    "# print(pos_df_no_comm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
